{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "P24_RNN_Paper_dplstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D79bF5DCrFv_"
      },
      "source": [
        "# Importing Important libraries \n",
        "\n",
        "# For data cleaning and visualization\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from datetime import date, datetime, timedelta\n",
        "\n",
        "\n",
        "# For model\n",
        "from numpy import newaxis\n",
        "import keras\n",
        "from keras import optimizers, callbacks\n",
        "from keras.layers import InputLayer, Input, Masking, Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import optimizers, callbacks\n",
        "\n",
        "from keras.layers import Dense, SimpleRNN, GRU\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# For saving the model\n",
        "import pickle\n",
        "\n",
        "# For Prediction \n",
        "from numpy import newaxis\n",
        "\n",
        "# For model Evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# For Plotting \n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV8gJOvTWH8s"
      },
      "source": [
        "def create_windows(data, data_len, sequence_len=10):\n",
        "  data_windows = []\n",
        "  for i in range(data_len - sequence_len):\n",
        "    data_windows.append(data[i : i+sequence_len])\n",
        "\n",
        "  # set the type of training data     \n",
        "  data_windows = np.array(data_windows).astype(float)\n",
        "  return (data_windows)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOEUJu-GWK3U"
      },
      "source": [
        "def Normalize_data(data_windows):\n",
        "  # number of windows formed \n",
        "  windows_no = data_windows.shape[0]\n",
        "  cols_no = data_windows.shape[2]\n",
        "\n",
        "  # initializing list to store normalized data\n",
        "  normalized_data = []\n",
        "  record_min=[]\n",
        "  record_max=[]\n",
        "\n",
        "  # normalizing begins\n",
        "  for win_index in range(windows_no):\n",
        "    normalized_window = []\n",
        "\n",
        "    for col_index in range(0,1):\n",
        "      # temporary column \n",
        "      t_col = data_windows[win_index, :, col_index]\n",
        "      t_min = min(t_col)\n",
        "      if (col_index == 0):\n",
        "        record_min.append(t_min)\n",
        "      t_col = t_col - t_min      \n",
        "      t_max = max(t_col)\n",
        "      if (col_index == 0):\n",
        "        record_max.append(t_max)\n",
        "      t_col = t_col/t_max\n",
        "      normalized_window.append(t_col)\n",
        "    \n",
        "    for col_index in range(1,  cols_no):\n",
        "      t_col = data_windows[win_index, :, col_index]\n",
        "      normalized_window.append(t_col)\n",
        "\n",
        "    normalized_window = np.array(normalized_window).T\n",
        "    normalized_data.append(normalized_window)\n",
        "\n",
        "  normalized_data=np.array(normalized_data)\n",
        "  return (normalized_data, record_max, record_min)\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdH-S0VjXMeV"
      },
      "source": [
        "def plot_training_loss(model_hist):\n",
        "  # plotting the lose curve during model training\n",
        "  plt.plot(model_hist.history['loss'])\n",
        "  plt.title('Training Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train'],loc='upper left')\n",
        "  plt.show()\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT_C_z1WY0oV"
      },
      "source": [
        "def Model_Evaluation(actual_prices, predicted_prices):\n",
        "  # Mean Absolute Error \n",
        "  MAE = metrics.mean_absolute_error(actual_prices, predicted_prices)\n",
        "  # Mean Squared Error\n",
        "  MSE = metrics.mean_squared_error(actual_prices, predicted_prices)\n",
        "  # Root Mean Squared Error\n",
        "  RMSE = np.sqrt(metrics.mean_squared_error(actual_prices, predicted_prices))\n",
        "\n",
        "  # Mean Absolute Percentage Error in degrees\n",
        "  errors = abs(actual_prices - predicted_prices)\n",
        "  MAPE = 100 * (errors /actual_prices)\n",
        "\n",
        "  # Model Accuracy\n",
        "  Accuracy = 100 - np.mean(MAPE)\n",
        "  return (Accuracy, MAE, MSE, RMSE)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haTPcsALVL4r"
      },
      "source": [
        "def DP(df):\n",
        "  # Adding noise to dataset\n",
        "  # calculating the variance of the mean_compound column\n",
        "  wsj_var=np.var(df.wsj_mean_compound)\n",
        "  cnbc_var=np.var(df.cnbc_mean_compound)\n",
        "  fortune_var=np.var(df.fortune_mean_compound)\n",
        "  reuters_var=np.var(df.reuters_mean_compound)\n",
        "\n",
        "  mu=0\n",
        "  noise=0.1\n",
        "  sigma_wsj=noise*wsj_var\n",
        "  sigma_cnbc=noise*cnbc_var\n",
        "  sigma_fortune=noise*fortune_var\n",
        "  sigma_reuters=noise*reuters_var\n",
        "\n",
        "  n=df.shape[0]\n",
        "  df_noise=pd.DataFrame()\n",
        "  df_noise['wsj_noise']=df['wsj_mean_compound']\n",
        "  df_noise['cnbc_noise']=df['cnbc_mean_compound']\n",
        "  df_noise['fortune_noise']=df['fortune_mean_compound']\n",
        "  df_noise['reuters_noise']=df['reuters_mean_compound']\n",
        "\n",
        "  for i in range(0,n):\n",
        "    df_noise['wsj_noise'][i]+=np.random.normal(mu,sigma_wsj)\n",
        "    df_noise['cnbc_noise'][i]+=np.random.normal(mu,sigma_cnbc)\n",
        "    df_noise['fortune_noise'][i]+=np.random.normal(mu,sigma_fortune)\n",
        "    df_noise['reuters_noise'][i]+=np.random.normal(mu,sigma_reuters)\n",
        "\n",
        "  df_noise.to_csv(\"source_price_noise0.csv\")\n",
        "  dfn=pd.read_csv(\"source_price_noise0.csv\",index_col=0)  \n",
        "\n",
        "  df_1n=pd.DataFrame()\n",
        "  df_1n['wsj']=dfn['wsj_noise']\n",
        "  df_1n['cnbc']=df['cnbc_mean_compound']\n",
        "  df_1n['fortune']=df['fortune_mean_compound']\n",
        "  df_1n['reuters']=df['reuters_mean_compound']\n",
        "  df_1n['price']=df['Adj Close']\n",
        "\n",
        "  df_2n=pd.DataFrame()\n",
        "  df_2n['wsj']=df['wsj_mean_compound']\n",
        "  df_2n['cnbc']=dfn['cnbc_noise']\n",
        "  df_2n['fortune']=df['fortune_mean_compound']\n",
        "  df_2n['reuters']=df['reuters_mean_compound']\n",
        "  df_2n['price']=df['Adj Close']\n",
        "\n",
        "  df_3n=pd.DataFrame()\n",
        "  df_3n['wsj']=df['wsj_mean_compound']\n",
        "  df_3n['cnbc']=df['cnbc_mean_compound']\n",
        "  df_3n['fortune']=dfn['fortune_noise']\n",
        "  df_3n['reuters']=df['reuters_mean_compound']\n",
        "  df_3n['price']=df['Adj Close']\n",
        "\n",
        "  df_4n=pd.DataFrame()\n",
        "  df_4n['wsj']=df['wsj_mean_compound']\n",
        "  df_4n['cnbc']=df['cnbc_mean_compound']\n",
        "  df_4n['fortune']=df['fortune_mean_compound']\n",
        "  df_4n['reuters']=dfn['reuters_noise']\n",
        "  df_4n['price']=df['Adj Close']\n",
        "\n",
        "  df1=df_1n\n",
        "  df2=df_2n\n",
        "  df3=df_3n\n",
        "  df4=df_4n\n",
        "\n",
        "  sequence_length=10;\n",
        "  split = 0.85\n",
        "  i_split = int(len(df1) * split)\n",
        "  i_split  \n",
        "\n",
        "  # choosing columns for the model\n",
        "  cols = ['price','wsj','cnbc','fortune','reuters']\n",
        "  data_train1 = df1.get(cols).values[:i_split]\n",
        "  data_train2 = df2.get(cols).values[:i_split]\n",
        "  data_train3 = df3.get(cols).values[:i_split]\n",
        "  data_train4 = df4.get(cols).values[:i_split]\n",
        "  train_len  = len(data_train1)\n",
        "  train_windows_len = None\n",
        "\n",
        "  print(\"data_train = \",data_train1.shape)\n",
        "  # Normalizing Train data1\n",
        "  data_windows1 = create_windows(data_train1,train_len,sequence_length)\n",
        "  print(\"training: \")\n",
        "  #print(data_windows1)\n",
        "  normalized_data1,rmax_t1, rmin_t1 = Normalize_data(data_windows1)\n",
        "  print(normalized_data1)\n",
        "\n",
        "  x_train1 = normalized_data1[:, :-1]\n",
        "  y_train1 = normalized_data1[:, -1,[0]]\n",
        "\n",
        "  # Normalizing Train data2\n",
        "  data_windows2 = create_windows(data_train2, train_len,sequence_length)\n",
        "  normalized_data2,rmax_t2, rmin_t2 = Normalize_data(data_windows2)\n",
        "\n",
        "  x_train2 = normalized_data2[:, :-1]\n",
        "  y_train2 = normalized_data2[:, -1,[0]]\n",
        "\n",
        "  # Normalizing Train data3\n",
        "  data_windows3 = create_windows(data_train3, train_len,sequence_length)\n",
        "  normalized_data3,rmax_t3, rmin_t3 = Normalize_data(data_windows3)\n",
        "\n",
        "  x_train3 = normalized_data3[:, :-1]\n",
        "  y_train3 = normalized_data3[:, -1,[0]]\n",
        "\n",
        "  # Normalizing Train data4\n",
        "  data_windows4 = create_windows(data_train4, train_len,sequence_length)\n",
        "  normalized_data4,rmax_t4, rmin_t4 = Normalize_data(data_windows4)\n",
        "\n",
        "  x_train4 = normalized_data4[:, :-1]\n",
        "  y_train4 = normalized_data4[:, -1,[0]]\n",
        "\n",
        "  # concatenating the training data\n",
        "  x_train = np.concatenate((x_train1,x_train2, x_train3, x_train4),axis=0)\n",
        "  y_train = np.concatenate((y_train1,y_train2, y_train3, y_train4),axis=0)\n",
        "\n",
        "\n",
        "\n",
        "  # Creating the Test Data\n",
        "  df_test = df\n",
        "  df_test.columns=['wsj','cnbc','fortune','reuters','price','date']\n",
        "  cols = ['price','wsj','cnbc','fortune','reuters']\n",
        "  cols2 = ['date']\n",
        "  test_len = df_test.shape[0]\n",
        "  data_test  = df_test.get(cols).values[i_split:]\n",
        "  data_test2  = df_test.get(cols2).values[i_split:]\n",
        "\n",
        "  data_windows_test = create_windows(data_test, len(data_test), sequence_length)\n",
        "\n",
        "  # the last price value of each window is the original y_test\n",
        "  y_test_original = data_windows_test[:, -1, [0]]\n",
        "  normalized_data_test, record_max_test, record_min_test = Normalize_data(data_windows_test)\n",
        "\n",
        "  x_test = normalized_data_test[:, :-1]\n",
        "  y_test = normalized_data_test[:, -1,[0]]\n",
        "\n",
        "\n",
        "  input_dim=x_train.shape[2] #2\n",
        "  input_timesteps=x_train.shape[1] #9\n",
        "\n",
        "\n",
        "  # Simple RNN model\n",
        "  model = Sequential()\n",
        "  model.add(SimpleRNN(units = 60, activation = 'relu',return_sequences = True,input_shape=(input_timesteps, input_dim)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(SimpleRNN(units = 60,activation = 'relu', return_sequences = True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(SimpleRNN(units = 80, activation = 'relu',return_sequences = True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(SimpleRNN(units = 120,activation = 'relu', return_sequences = True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(units = 1))\n",
        "  model.compile(optimizer='adam', loss = 'mean_squared_error', metrics=['mean_squared_error'])\n",
        "  hist = model.fit(x_train, y_train, epochs=25, batch_size=32)\n",
        "\n",
        "\n",
        "  plot_training_loss(hist)\n",
        "\n",
        "\n",
        "  # Prediction of Test Data using the Training Model\n",
        "  # Using the trained model for prediction and check the performance metrics\n",
        "  train_predict = model.predict(x_train)\n",
        "  test_predict = model.predict(x_test)\n",
        "  # Multi-sequence Prediction\n",
        "  # predicting x_test\n",
        "  prediction_len = 1\n",
        "  # x_test needs to be predicted \n",
        "  data = x_test\n",
        "  predicted_vals = []\n",
        "  window_size = sequence_length\n",
        "  pre_win_no = int(len(data)/prediction_len)\n",
        "  for i in range(pre_win_no):\n",
        "    # access x_test window by window\n",
        "    curr_frame = data[i*prediction_len]\n",
        "    pred = []\n",
        "    for j in range(prediction_len):\n",
        "      # increase the dimension of current frame by one using newaxis, so that it can be fed to model for prediction\n",
        "      model_predict = model.predict(curr_frame[newaxis,:,:])[0]\n",
        "      pred.append(model_predict)\n",
        "      # shift the current frame forward\n",
        "      curr_frame = curr_frame[1:]\n",
        "      # insert the currently predicted value in the frame\n",
        "      # add the new predicted value at the end of window frame \n",
        "      curr_frame = np.insert(curr_frame, [window_size-2], pred[-1], axis=0)\n",
        "    predicted_vals.append(pred)\n",
        "  print(len(predicted_vals))\n",
        "  # Denormalizing the Prediction Results to get Predicted Adj Close Price\n",
        "\n",
        "  pred_prices = []\n",
        "  len_pre_win = int(len(data)/prediction_len)\n",
        "  cnt=0\n",
        "  for i in range(0,len_pre_win):\n",
        "      for j in range(0,prediction_len):\n",
        "        pred_prices.append(predicted_vals[i][j][0]*record_max_test[cnt]+record_min_test[cnt])\n",
        "        cnt = cnt+1\n",
        "\n",
        "\n",
        "  # Comparing Actual and Predicted Prices\n",
        "  actual_prices = []\n",
        "  for i in y_test_original.tolist():\n",
        "    actual_prices.append(i[0])\n",
        "  dates = []\n",
        "  for i in data_test2.tolist():\n",
        "    dates.append(i[0])\n",
        "  dates = dates[len(dates)-len(actual_prices):]\n",
        "  res = { 'date':dates,\n",
        "        'Actual': actual_prices,\n",
        "        'Predicted': pred_prices\n",
        "        }\n",
        "  df_compare = pd.DataFrame(res,columns = ['date','Actual','Predicted'])\n",
        "  print(df_compare)\n",
        "  # Performance Evaluation\n",
        "  actual_prices = df_compare['Actual']\n",
        "  predicted_prices = df_compare['Predicted']  \n",
        "\n",
        "  Accuracy, MAE, MSE, RMSE = Model_Evaluation(actual_prices, predicted_prices)\n",
        "  print(\"\\n-----Model Evaluation-----------------------------------------------------\\n\")\n",
        "  print(\"LSTM Model Loss = \", model.evaluate(x_test, y_test, verbose = 2))\n",
        "  print(\"Model Accuracy = \", Accuracy)\n",
        "  print(\"Mean Absolute Error = \", MAE,\" degrees\")\n",
        "  print(\"Mean Squared Error = \", MSE)\n",
        "  print(\"Root Mean Squared Error = \", RMSE)\n",
        "  print(\"\\n--------------------------------------------------------------------------\\n\")\n",
        "\n",
        " \n",
        "  return (hist, model, df_compare, Accuracy, MAE, MSE, RMSE)\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "O2HfUBb5V3mR",
        "outputId": "a75ac1b3-7a85-4da4-869f-b82413de8350"
      },
      "source": [
        "df_name = \"source_price.csv\"\n",
        "df = pd.read_csv(df_name, index_col=0)\n",
        "df['date'] = df.index\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wsj_mean_compound</th>\n",
              "      <th>cnbc_mean_compound</th>\n",
              "      <th>fortune_mean_compound</th>\n",
              "      <th>reuters_mean_compound</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.296000</td>\n",
              "      <td>-0.136600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2636.979980</td>\n",
              "      <td>2017/12/7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.242300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2651.500000</td>\n",
              "      <td>2017/12/8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2659.989990</td>\n",
              "      <td>2017/12/11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2664.110107</td>\n",
              "      <td>2017/12/12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2662.850098</td>\n",
              "      <td>2017/12/13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>0.030290</td>\n",
              "      <td>0.047433</td>\n",
              "      <td>0.011550</td>\n",
              "      <td>-0.025190</td>\n",
              "      <td>2721.330078</td>\n",
              "      <td>2018/5/25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>-0.052796</td>\n",
              "      <td>0.070442</td>\n",
              "      <td>-0.025721</td>\n",
              "      <td>-0.035568</td>\n",
              "      <td>2689.860107</td>\n",
              "      <td>2018/5/29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>-0.017367</td>\n",
              "      <td>0.038119</td>\n",
              "      <td>-0.076965</td>\n",
              "      <td>-0.063177</td>\n",
              "      <td>2724.010010</td>\n",
              "      <td>2018/5/30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>-0.018636</td>\n",
              "      <td>0.057371</td>\n",
              "      <td>-0.064138</td>\n",
              "      <td>-0.025489</td>\n",
              "      <td>2705.270020</td>\n",
              "      <td>2018/5/31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.061150</td>\n",
              "      <td>0.361200</td>\n",
              "      <td>-0.004489</td>\n",
              "      <td>2734.620117</td>\n",
              "      <td>2018/6/1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>121 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     wsj_mean_compound  cnbc_mean_compound  ...    Adj Close        date\n",
              "0             0.296000           -0.136600  ...  2636.979980   2017/12/7\n",
              "1             0.000000            0.000000  ...  2651.500000   2017/12/8\n",
              "2             0.000000            0.000000  ...  2659.989990  2017/12/11\n",
              "3             0.000000            0.000000  ...  2664.110107  2017/12/12\n",
              "4             0.000000            0.000000  ...  2662.850098  2017/12/13\n",
              "..                 ...                 ...  ...          ...         ...\n",
              "116           0.030290            0.047433  ...  2721.330078   2018/5/25\n",
              "117          -0.052796            0.070442  ...  2689.860107   2018/5/29\n",
              "118          -0.017367            0.038119  ...  2724.010010   2018/5/30\n",
              "119          -0.018636            0.057371  ...  2705.270020   2018/5/31\n",
              "120           0.000000           -0.061150  ...  2734.620117    2018/6/1\n",
              "\n",
              "[121 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkWNOIZ9V6qi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9157c8d7-8482-467a-a549-fbb2f902a5ef"
      },
      "source": [
        "hist, model, df_compare, Accuracy, MAE, MSE, RMSE = DP(df)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_train =  (102, 5)\n",
            "training: \n",
            "[[[ 0.00000000e+00  2.96207808e-01 -1.36600000e-01  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  [ 2.73035701e-01 -3.28338312e-04  0.00000000e+00 -2.42300000e-01\n",
            "    0.00000000e+00]\n",
            "  [ 4.32682200e-01  2.19966470e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  ...\n",
            "  [ 1.00000000e+00 -1.49776326e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  [ 8.36593605e-01 -4.94653862e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  [ 7.94849080e-01  4.08823523e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]]\n",
            "\n",
            " [[ 0.00000000e+00 -3.28338312e-04  0.00000000e+00 -2.42300000e-01\n",
            "    0.00000000e+00]\n",
            "  [ 2.19607070e-01  2.19966470e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  [ 3.26180437e-01 -5.58350316e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  ...\n",
            "  [ 7.75220880e-01 -4.94653862e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  [ 7.17797806e-01  4.08823523e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  [ 8.55409811e-01  6.68460395e-02  2.96000000e-01  0.00000000e+00\n",
            "    0.00000000e+00]]\n",
            "\n",
            " [[ 2.09174325e-01  2.19966470e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  [ 3.17172427e-01 -5.58350316e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  [ 2.84144583e-01  1.85461652e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  ...\n",
            "  [ 7.14025163e-01  4.08823523e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  [ 8.53476845e-01  6.68460395e-02  2.96000000e-01  0.00000000e+00\n",
            "    0.00000000e+00]\n",
            "  [ 8.21236133e-01 -6.11515064e-04  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.00000000e+00 -7.63164580e-03  4.73992380e-02  7.01941180e-02\n",
            "    4.95155490e-02]\n",
            "  [ 7.90631145e-01 -2.68597952e-02  5.64761620e-02 -6.55000000e-03\n",
            "    5.04848250e-02]\n",
            "  [ 4.80290412e-01 -7.02652532e-02  4.20934260e-02  3.21739130e-02\n",
            "    3.73642970e-02]\n",
            "  ...\n",
            "  [ 4.77185910e-01  1.13310649e-02  8.62676560e-02  2.90113640e-02\n",
            "    1.07366484e-01]\n",
            "  [ 1.82100705e-01 -9.07158482e-03  6.40564320e-02  2.13281250e-02\n",
            "    6.60761150e-02]\n",
            "  [ 2.73218620e-01 -4.47314345e-02  8.42461290e-02 -1.32905660e-02\n",
            "    6.89561960e-02]]\n",
            "\n",
            " [[ 1.00000000e+00 -2.68597952e-02  5.64761620e-02 -6.55000000e-03\n",
            "    5.04848250e-02]\n",
            "  [ 6.07477222e-01 -7.02652532e-02  4.20934260e-02  3.21739130e-02\n",
            "    3.73642970e-02]\n",
            "  [ 6.10040761e-01  3.94114753e-02  6.61267220e-02 -1.79214290e-02\n",
            "   -2.05982140e-02]\n",
            "  ...\n",
            "  [ 2.30323212e-01 -9.07158482e-03  6.40564320e-02  2.13281250e-02\n",
            "    6.60761150e-02]\n",
            "  [ 3.45570272e-01 -4.47314345e-02  8.42461290e-02 -1.32905660e-02\n",
            "    6.89561960e-02]\n",
            "  [ 1.89493996e-02  4.96042983e-04  8.80572530e-02  4.33389830e-02\n",
            "    5.37059060e-02]]\n",
            "\n",
            " [[ 9.96298181e-01 -7.02652532e-02  4.20934260e-02  3.21739130e-02\n",
            "    3.73642970e-02]\n",
            "  [ 1.00000000e+00  3.94114753e-02  6.61267220e-02 -1.79214290e-02\n",
            "   -2.05982140e-02]\n",
            "  [ 1.19084615e-01  7.47232038e-02 -1.65783130e-02 -1.70000000e-01\n",
            "    1.32029117e-01]\n",
            "  ...\n",
            "  [ 6.18097449e-01 -4.47314345e-02  8.42461290e-02 -1.32905660e-02\n",
            "    6.89561960e-02]\n",
            "  [ 1.46448061e-01  4.96042983e-04  8.80572530e-02  4.33389830e-02\n",
            "    5.37059060e-02]\n",
            "  [ 0.00000000e+00  1.69539458e-02  6.64606540e-02  6.31477270e-02\n",
            "    6.80718100e-02]]]\n",
            "Epoch 1/25\n",
            "12/12 [==============================] - 3s 13ms/step - loss: 0.3610 - mean_squared_error: 0.3610\n",
            "Epoch 2/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.2116 - mean_squared_error: 0.2116\n",
            "Epoch 3/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.1330 - mean_squared_error: 0.1330\n",
            "Epoch 4/25\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.1096 - mean_squared_error: 0.1096\n",
            "Epoch 5/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.1001 - mean_squared_error: 0.1001\n",
            "Epoch 6/25\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0906 - mean_squared_error: 0.0906\n",
            "Epoch 7/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0924 - mean_squared_error: 0.0924\n",
            "Epoch 8/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0853 - mean_squared_error: 0.0853\n",
            "Epoch 9/25\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0826 - mean_squared_error: 0.0826\n",
            "Epoch 10/25\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0747 - mean_squared_error: 0.0747\n",
            "Epoch 11/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0780 - mean_squared_error: 0.0780\n",
            "Epoch 12/25\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0788 - mean_squared_error: 0.0788\n",
            "Epoch 13/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0721 - mean_squared_error: 0.0721\n",
            "Epoch 14/25\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0706 - mean_squared_error: 0.0706\n",
            "Epoch 15/25\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0713 - mean_squared_error: 0.0713\n",
            "Epoch 16/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0625 - mean_squared_error: 0.0625\n",
            "Epoch 17/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0538 - mean_squared_error: 0.0538\n",
            "Epoch 18/25\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0600 - mean_squared_error: 0.0600\n",
            "Epoch 19/25\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0561 - mean_squared_error: 0.0561\n",
            "Epoch 20/25\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0537 - mean_squared_error: 0.0537\n",
            "Epoch 21/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0547 - mean_squared_error: 0.0547\n",
            "Epoch 22/25\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0478 - mean_squared_error: 0.0478\n",
            "Epoch 23/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0489 - mean_squared_error: 0.0489\n",
            "Epoch 24/25\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0490 - mean_squared_error: 0.0490\n",
            "Epoch 25/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0427 - mean_squared_error: 0.0427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "        date       Actual    Predicted\n",
            "0  2018/5/21  2712.969971  [2719.4126]\n",
            "1  2018/5/22  2733.010010  [2717.0415]\n",
            "2  2018/5/23  2724.439941  [2726.4692]\n",
            "3  2018/5/24  2733.290039  [2720.9436]\n",
            "4  2018/5/25  2727.760010  [2719.3992]\n",
            "5  2018/5/29  2721.330078   [2719.439]\n",
            "6  2018/5/30  2689.860107  [2708.8079]\n",
            "7  2018/5/31  2724.010010   [2708.094]\n",
            "8   2018/6/1  2705.270020  [2709.3289]\n",
            "\n",
            "-----Model Evaluation-----------------------------------------------------\n",
            "\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_test_function.<locals>.test_function at 0x7f6f33b5e290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 - 1s - loss: 0.1831 - mean_squared_error: 0.1831\n",
            "LSTM Model Loss =  [0.1830788403749466, 0.1830788403749466]\n",
            "Model Accuracy =  [99.64859]\n",
            "Mean Absolute Error =  9.551269595486096  degrees\n",
            "Mean Squared Error =  128.37157929302964\n",
            "Root Mean Squared Error =  11.330118238263431\n",
            "\n",
            "--------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}