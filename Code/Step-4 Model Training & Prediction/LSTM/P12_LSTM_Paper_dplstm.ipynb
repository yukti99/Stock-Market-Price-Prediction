{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "P12_LSTM_Paper_dplstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D79bF5DCrFv_"
      },
      "source": [
        "# Importing Important libraries \n",
        "\n",
        "# For data cleaning and visualization\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from datetime import date, datetime, timedelta\n",
        "\n",
        "\n",
        "# For model\n",
        "from numpy import newaxis\n",
        "import keras\n",
        "from keras import optimizers, callbacks\n",
        "from keras.layers import InputLayer, Input, Masking, Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import optimizers, callbacks\n",
        "\n",
        "# For saving the model\n",
        "import pickle\n",
        "\n",
        "# For Prediction \n",
        "from numpy import newaxis\n",
        "\n",
        "# For model Evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# For Plotting \n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams "
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV8gJOvTWH8s"
      },
      "source": [
        "def create_windows(data, data_len, sequence_len=10):\n",
        "  data_windows = []\n",
        "  for i in range(data_len - sequence_len):\n",
        "    data_windows.append(data[i : i+sequence_len])\n",
        "\n",
        "  # set the type of training data     \n",
        "  data_windows = np.array(data_windows).astype(float)\n",
        "  return (data_windows)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOEUJu-GWK3U"
      },
      "source": [
        "def Normalize_data(data_windows):\n",
        "  # number of windows formed \n",
        "  windows_no = data_windows.shape[0]\n",
        "  cols_no = data_windows.shape[2]\n",
        "\n",
        "  # initializing list to store normalized data\n",
        "  normalized_data = []\n",
        "  record_min=[]\n",
        "  record_max=[]\n",
        "\n",
        "  # normalizing begins\n",
        "  for win_index in range(windows_no):\n",
        "    normalized_window = []\n",
        "\n",
        "    for col_index in range(0,1):\n",
        "      # temporary column \n",
        "      t_col = data_windows[win_index, :, col_index]\n",
        "      t_min = min(t_col)\n",
        "      if (col_index == 0):\n",
        "        record_min.append(t_min)\n",
        "      t_col = t_col - t_min      \n",
        "      t_max = max(t_col)\n",
        "      if (col_index == 0):\n",
        "        record_max.append(t_max)\n",
        "      t_col = t_col/t_max\n",
        "      normalized_window.append(t_col)\n",
        "    \n",
        "    for col_index in range(1,  cols_no):\n",
        "      t_col = data_windows[win_index, :, col_index]\n",
        "      normalized_window.append(t_col)\n",
        "\n",
        "    normalized_window = np.array(normalized_window).T\n",
        "    normalized_data.append(normalized_window)\n",
        "\n",
        "  normalized_data=np.array(normalized_data)\n",
        "  return (normalized_data, record_max, record_min)\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdH-S0VjXMeV"
      },
      "source": [
        "def plot_training_loss(model_hist):\n",
        "  # plotting the lose curve during model training\n",
        "  plt.plot(model_hist.history['loss'])\n",
        "  plt.title('Training Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train'],loc='upper left')\n",
        "  plt.show()\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT_C_z1WY0oV"
      },
      "source": [
        "def Model_Evaluation(actual_prices, predicted_prices):\n",
        "  # Mean Absolute Error \n",
        "  MAE = metrics.mean_absolute_error(actual_prices, predicted_prices)\n",
        "  # Mean Squared Error\n",
        "  MSE = metrics.mean_squared_error(actual_prices, predicted_prices)\n",
        "  # Root Mean Squared Error\n",
        "  RMSE = np.sqrt(metrics.mean_squared_error(actual_prices, predicted_prices))\n",
        "\n",
        "  # Mean Absolute Percentage Error in degrees\n",
        "  errors = abs(actual_prices - predicted_prices)\n",
        "  MAPE = 100 * (errors /actual_prices)\n",
        "\n",
        "  # Model Accuracy\n",
        "  Accuracy = 100 - np.mean(MAPE)\n",
        "  return (Accuracy, MAE, MSE, RMSE)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haTPcsALVL4r"
      },
      "source": [
        "def DP_LSTM(df):\n",
        "  # Adding noise to dataset\n",
        "  # calculating the variance of the mean_compound column\n",
        "  wsj_var=np.var(df.wsj_mean_compound)\n",
        "  cnbc_var=np.var(df.cnbc_mean_compound)\n",
        "  fortune_var=np.var(df.fortune_mean_compound)\n",
        "  reuters_var=np.var(df.reuters_mean_compound)\n",
        "\n",
        "  mu=0\n",
        "  noise=0.1\n",
        "  sigma_wsj=noise*wsj_var\n",
        "  sigma_cnbc=noise*cnbc_var\n",
        "  sigma_fortune=noise*fortune_var\n",
        "  sigma_reuters=noise*reuters_var\n",
        "\n",
        "  n=df.shape[0]\n",
        "  df_noise=pd.DataFrame()\n",
        "  df_noise['wsj_noise']=df['wsj_mean_compound']\n",
        "  df_noise['cnbc_noise']=df['cnbc_mean_compound']\n",
        "  df_noise['fortune_noise']=df['fortune_mean_compound']\n",
        "  df_noise['reuters_noise']=df['reuters_mean_compound']\n",
        "\n",
        "  for i in range(0,n):\n",
        "    df_noise['wsj_noise'][i]+=np.random.normal(mu,sigma_wsj)\n",
        "    df_noise['cnbc_noise'][i]+=np.random.normal(mu,sigma_cnbc)\n",
        "    df_noise['fortune_noise'][i]+=np.random.normal(mu,sigma_fortune)\n",
        "    df_noise['reuters_noise'][i]+=np.random.normal(mu,sigma_reuters)\n",
        "\n",
        "  df_noise.to_csv(\"source_price_noise0.csv\")\n",
        "  dfn=pd.read_csv(\"source_price_noise0.csv\",index_col=0)  \n",
        "\n",
        "  df_1n=pd.DataFrame()\n",
        "  df_1n['wsj']=dfn['wsj_noise']\n",
        "  df_1n['cnbc']=df['cnbc_mean_compound']\n",
        "  df_1n['fortune']=df['fortune_mean_compound']\n",
        "  df_1n['reuters']=df['reuters_mean_compound']\n",
        "  df_1n['price']=df['Adj Close']\n",
        "\n",
        "  df_2n=pd.DataFrame()\n",
        "  df_2n['wsj']=df['wsj_mean_compound']\n",
        "  df_2n['cnbc']=dfn['cnbc_noise']\n",
        "  df_2n['fortune']=df['fortune_mean_compound']\n",
        "  df_2n['reuters']=df['reuters_mean_compound']\n",
        "  df_2n['price']=df['Adj Close']\n",
        "\n",
        "  df_3n=pd.DataFrame()\n",
        "  df_3n['wsj']=df['wsj_mean_compound']\n",
        "  df_3n['cnbc']=df['cnbc_mean_compound']\n",
        "  df_3n['fortune']=dfn['fortune_noise']\n",
        "  df_3n['reuters']=df['reuters_mean_compound']\n",
        "  df_3n['price']=df['Adj Close']\n",
        "\n",
        "  df_4n=pd.DataFrame()\n",
        "  df_4n['wsj']=df['wsj_mean_compound']\n",
        "  df_4n['cnbc']=df['cnbc_mean_compound']\n",
        "  df_4n['fortune']=df['fortune_mean_compound']\n",
        "  df_4n['reuters']=dfn['reuters_noise']\n",
        "  df_4n['price']=df['Adj Close']\n",
        "\n",
        "  df1=df_1n\n",
        "  df2=df_2n\n",
        "  df3=df_3n\n",
        "  df4=df_4n\n",
        "\n",
        "  sequence_length=10;\n",
        "  split = 0.85\n",
        "  i_split = int(len(df1) * split)\n",
        "  i_split  \n",
        "\n",
        "  # choosing columns for the model\n",
        "  cols = ['price','wsj','cnbc','fortune','reuters']\n",
        "  data_train1 = df1.get(cols).values[:i_split]\n",
        "  data_train2 = df2.get(cols).values[:i_split]\n",
        "  data_train3 = df3.get(cols).values[:i_split]\n",
        "  data_train4 = df4.get(cols).values[:i_split]\n",
        "  train_len  = len(data_train1)\n",
        "  train_windows_len = None\n",
        "\n",
        "\n",
        "  # Normalizing Train data1\n",
        "  data_windows1 = create_windows(data_train1,train_len,sequence_length)\n",
        "  normalized_data1,rmax_t1, rmin_t1 = Normalize_data(data_windows1)\n",
        "\n",
        "\n",
        "  x_train1 = normalized_data1[:, :-1]\n",
        "  y_train1 = normalized_data1[:, -1,[0]]\n",
        "\n",
        "  # Normalizing Train data2\n",
        "  data_windows2 = create_windows(data_train2, train_len,sequence_length)\n",
        "  normalized_data2,rmax_t2, rmin_t2 = Normalize_data(data_windows2)\n",
        "\n",
        "  x_train2 = normalized_data2[:, :-1]\n",
        "  y_train2 = normalized_data2[:, -1,[0]]\n",
        "\n",
        "  # Normalizing Train data3\n",
        "  data_windows3 = create_windows(data_train3, train_len,sequence_length)\n",
        "  normalized_data3,rmax_t3, rmin_t3 = Normalize_data(data_windows3)\n",
        "\n",
        "  x_train3 = normalized_data3[:, :-1]\n",
        "  y_train3 = normalized_data3[:, -1,[0]]\n",
        "\n",
        "  # Normalizing Train data4\n",
        "  data_windows4 = create_windows(data_train4, train_len,sequence_length)\n",
        "  normalized_data4,rmax_t4, rmin_t4 = Normalize_data(data_windows4)\n",
        "\n",
        "  x_train4 = normalized_data4[:, :-1]\n",
        "  y_train4 = normalized_data4[:, -1,[0]]\n",
        "\n",
        "  # concatenating the training data\n",
        "  x_train = np.concatenate((x_train1,x_train2, x_train3, x_train4),axis=0)\n",
        "  y_train = np.concatenate((y_train1,y_train2, y_train3, y_train4),axis=0)\n",
        "\n",
        "\n",
        "\n",
        "  # Creating the Test Data\n",
        "  df_test = df\n",
        "  df_test.columns=['wsj','cnbc','fortune','reuters','price','date']\n",
        "  cols = ['price','wsj','cnbc','fortune','reuters']\n",
        "  cols2 = ['date']\n",
        "  test_len = df_test.shape[0]\n",
        "  data_test  = df_test.get(cols).values[i_split:]\n",
        "  data_test2  = df_test.get(cols2).values[i_split:]\n",
        "\n",
        "  data_windows_test = create_windows(data_test, len(data_test), sequence_length)\n",
        "\n",
        "  # the last price value of each window is the original y_test\n",
        "  y_test_original = data_windows_test[:, -1, [0]]\n",
        "  normalized_data_test, record_max_test, record_min_test = Normalize_data(data_windows_test)\n",
        "\n",
        "  x_test = normalized_data_test[:, :-1]\n",
        "  y_test = normalized_data_test[:, -1,[0]]\n",
        "\n",
        "\n",
        "  input_dim=x_train.shape[2] #2\n",
        "  input_timesteps=x_train.shape[1] #9\n",
        "\n",
        "  # LSTM Model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(units = 60, activation = 'relu', return_sequences = True,input_shape=(input_timesteps, input_dim)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units = 60, activation = 'relu', return_sequences = True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units = 80, activation = 'relu', return_sequences = True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units = 120, activation = 'relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(units = 1))\n",
        "  model.compile(optimizer='adam', loss = 'mean_squared_error', metrics=['mean_squared_error'])\n",
        "  hist = model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "\n",
        "  plot_training_loss(hist)\n",
        "\n",
        "\n",
        "  # Prediction of Test Data using the Training Model\n",
        "  # Using the trained model for prediction and check the performance metrics\n",
        "  train_predict = model.predict(x_train)\n",
        "  test_predict = model.predict(x_test)\n",
        "  # Multi-sequence Prediction\n",
        "  # predicting x_test\n",
        "  prediction_len = 1\n",
        "  # x_test needs to be predicted \n",
        "  data = x_test\n",
        "  predicted_vals = []\n",
        "  window_size = sequence_length\n",
        "  pre_win_no = int(len(data)/prediction_len)\n",
        "  for i in range(pre_win_no):\n",
        "    # access x_test window by window\n",
        "    curr_frame = data[i*prediction_len]\n",
        "    pred = []\n",
        "    for j in range(prediction_len):\n",
        "      # increase the dimension of current frame by one using newaxis, so that it can be fed to model for prediction\n",
        "      model_predict = model.predict(curr_frame[newaxis,:,:])[0]\n",
        "      pred.append(model_predict)\n",
        "      # shift the current frame forward\n",
        "      curr_frame = curr_frame[1:]\n",
        "      # insert the currently predicted value in the frame\n",
        "      # add the new predicted value at the end of window frame \n",
        "      curr_frame = np.insert(curr_frame, [window_size-2], pred[-1], axis=0)\n",
        "    predicted_vals.append(pred)\n",
        "  print(len(predicted_vals))\n",
        "  # Denormalizing the Prediction Results to get Predicted Adj Close Price\n",
        "\n",
        "  pred_prices = []\n",
        "  len_pre_win = int(len(data)/prediction_len)\n",
        "  cnt=0\n",
        "  for i in range(0,len_pre_win):\n",
        "      for j in range(0,prediction_len):\n",
        "        pred_prices.append(predicted_vals[i][j][0]*record_max_test[cnt]+record_min_test[cnt])\n",
        "        cnt = cnt+1\n",
        "\n",
        "\n",
        "  # Comparing Actual and Predicted Prices\n",
        "  actual_prices = []\n",
        "  for i in y_test_original.tolist():\n",
        "    actual_prices.append(i[0])\n",
        "  dates = []\n",
        "  for i in data_test2.tolist():\n",
        "    dates.append(i[0])\n",
        "  dates = dates[len(dates)-len(actual_prices):]\n",
        "  res = { 'date':dates,\n",
        "        'Actual': actual_prices,\n",
        "        'Predicted': pred_prices\n",
        "        }\n",
        "  df_compare = pd.DataFrame(res,columns = ['date','Actual','Predicted'])\n",
        "  print(df_compare)\n",
        "  # Performance Evaluation\n",
        "  actual_prices = df_compare['Actual']\n",
        "  predicted_prices = df_compare['Predicted']  \n",
        "\n",
        "  Accuracy, MAE, MSE, RMSE = Model_Evaluation(actual_prices, predicted_prices)\n",
        "  print(\"\\n-----Model Evaluation-----------------------------------------------------\\n\")\n",
        "  print(\"LSTM Model Loss = \", model.evaluate(x_test, y_test, verbose = 2))\n",
        "  print(\"Model Accuracy = \", Accuracy)\n",
        "  print(\"Mean Absolute Error = \", MAE,\" degrees\")\n",
        "  print(\"Mean Squared Error = \", MSE)\n",
        "  print(\"Root Mean Squared Error = \", RMSE)\n",
        "  print(\"\\n--------------------------------------------------------------------------\\n\")\n",
        "\n",
        " \n",
        "  return (hist, model, df_compare, Accuracy, MAE, MSE, RMSE)\n",
        "\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "O2HfUBb5V3mR",
        "outputId": "29dd5fc0-f099-4a5c-896e-c7310eb74920"
      },
      "source": [
        "df_name = \"source_price.csv\"\n",
        "df = pd.read_csv(df_name, index_col=0)\n",
        "df['date'] = df.index\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wsj_mean_compound</th>\n",
              "      <th>cnbc_mean_compound</th>\n",
              "      <th>fortune_mean_compound</th>\n",
              "      <th>reuters_mean_compound</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.296000</td>\n",
              "      <td>-0.136600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2636.979980</td>\n",
              "      <td>2017/12/7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.242300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2651.500000</td>\n",
              "      <td>2017/12/8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2659.989990</td>\n",
              "      <td>2017/12/11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2664.110107</td>\n",
              "      <td>2017/12/12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2662.850098</td>\n",
              "      <td>2017/12/13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>0.030290</td>\n",
              "      <td>0.047433</td>\n",
              "      <td>0.011550</td>\n",
              "      <td>-0.025190</td>\n",
              "      <td>2721.330078</td>\n",
              "      <td>2018/5/25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>-0.052796</td>\n",
              "      <td>0.070442</td>\n",
              "      <td>-0.025721</td>\n",
              "      <td>-0.035568</td>\n",
              "      <td>2689.860107</td>\n",
              "      <td>2018/5/29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>-0.017367</td>\n",
              "      <td>0.038119</td>\n",
              "      <td>-0.076965</td>\n",
              "      <td>-0.063177</td>\n",
              "      <td>2724.010010</td>\n",
              "      <td>2018/5/30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>-0.018636</td>\n",
              "      <td>0.057371</td>\n",
              "      <td>-0.064138</td>\n",
              "      <td>-0.025489</td>\n",
              "      <td>2705.270020</td>\n",
              "      <td>2018/5/31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.061150</td>\n",
              "      <td>0.361200</td>\n",
              "      <td>-0.004489</td>\n",
              "      <td>2734.620117</td>\n",
              "      <td>2018/6/1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>121 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     wsj_mean_compound  cnbc_mean_compound  ...    Adj Close        date\n",
              "0             0.296000           -0.136600  ...  2636.979980   2017/12/7\n",
              "1             0.000000            0.000000  ...  2651.500000   2017/12/8\n",
              "2             0.000000            0.000000  ...  2659.989990  2017/12/11\n",
              "3             0.000000            0.000000  ...  2664.110107  2017/12/12\n",
              "4             0.000000            0.000000  ...  2662.850098  2017/12/13\n",
              "..                 ...                 ...  ...          ...         ...\n",
              "116           0.030290            0.047433  ...  2721.330078   2018/5/25\n",
              "117          -0.052796            0.070442  ...  2689.860107   2018/5/29\n",
              "118          -0.017367            0.038119  ...  2724.010010   2018/5/30\n",
              "119          -0.018636            0.057371  ...  2705.270020   2018/5/31\n",
              "120           0.000000           -0.061150  ...  2734.620117    2018/6/1\n",
              "\n",
              "[121 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkWNOIZ9V6qi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        },
        "outputId": "de4174e2-1991-417a-8732-2ac57234b817"
      },
      "source": [
        "hist, model, df_compare, Accuracy, MAE, MSE, RMSE = DP_LSTM(df)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 4s 52ms/step - loss: 0.4196 - mean_squared_error: 0.4196\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 1s 51ms/step - loss: 0.2130 - mean_squared_error: 0.2130\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 1s 55ms/step - loss: 0.1622 - mean_squared_error: 0.1622\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 1s 56ms/step - loss: 0.1361 - mean_squared_error: 0.1361\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 1s 51ms/step - loss: 0.1179 - mean_squared_error: 0.1179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "        date       Actual    Predicted\n",
            "0  2018/5/21  2712.969971  2712.645043\n",
            "1  2018/5/22  2733.010010  2705.785923\n",
            "2  2018/5/23  2724.439941  2717.276041\n",
            "3  2018/5/24  2733.290039  2722.926889\n",
            "4  2018/5/25  2727.760010  2723.470770\n",
            "5  2018/5/29  2721.330078  2724.976041\n",
            "6  2018/5/30  2689.860107  2708.739323\n",
            "7  2018/5/31  2724.010010  2707.840782\n",
            "8   2018/6/1  2705.270020  2707.900246\n",
            "\n",
            "-----Model Evaluation-----------------------------------------------------\n",
            "\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fe5102150e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 - 0s - loss: 0.0959 - mean_squared_error: 0.0959\n",
            "LSTM Model Loss =  [0.09587074816226959, 0.09587074816226959]\n",
            "Model Accuracy =  99.62954576129046\n",
            "Mean Absolute Error =  10.076659810589515  degrees\n",
            "Mean Squared Error =  172.93891671369246\n",
            "Root Mean Squared Error =  13.150624194831684\n",
            "\n",
            "--------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}